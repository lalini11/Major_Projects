# -*- coding: utf-8 -*-
"""major.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UZh27yAdGVqMMFXPNZjF6mU9eq5eFhgJ
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

df = pd.read_csv('/content/drive/MyDrive/creditcard.csv')
df.head()

# statistical info
df.describe()

df.shape

"""#Data Preprocessing"""

# datatype info
df.info()

"""Checking the no of missing values in each column"""

df.isnull().sum()

"""Distribution of legit transaction and fraudulant transaction"""

df['Class'].value_counts()

"""Show no of legit and fraud transaction, where
0 --> legit transaction, 
1 --> Fraud Transaction
"""

legit=df[df.Class == 0]
fraud=df[df.Class == 1]
print(legit.shape)
print(fraud.shape)

"""Statistical Measures of the data"""

legit.Amount.describe()

fraud.Amount.describe()

"""Compare the values for both transaction"""

df.groupby('Class').mean()

"""UNDER SAMPLING 
Build a sample dataset witch containing similar distribution of normal and fraudulent transaction
"""

legit_sample=legit.sample(n=492)

"""Concatenating two dataframe"""

new_df=pd.concat([legit_sample, fraud], axis=0)

new_df.head()

#distribution of both transaction
new_df['Class'].value_counts()

#compare the value of both transaction 
new_df.groupby('Class').mean()

"""#Exploratory Data Analysis"""

sns.countplot(df['Class'])

sns.distplot(df['Time'])

"""#Without Sampling"""

X = df.drop(columns=['Class'], axis=1)
y = df['Class']

"""Spliting the dataset into Training set and Testing set"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size = 1/3, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
#trainig
classifier.fit(X_train, y_train)
#testing
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
# training
model.fit(X_train, y_train)
# testing
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
# training
model.fit(X_train, y_train)
# testing
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from xgboost import XGBClassifier
model = XGBClassifier(n_jobs=-1)
# training
model.fit(X_train, y_train)
# testing
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

"""#Feature Scalling then Split"""

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
standard_data = sc.fit_transform(X)

X=standard_data

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size = 1/3, random_state = 0)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
#trainig
classifier.fit(X_train, y_train)
#testing
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

"""#Split Data into Feature and Target(Sampled Dataset)"""

X = new_df.drop(columns=['Class'], axis=1)
y = new_df['Class']

print(X)

print(y)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
standard_data = sc.fit_transform(X)

print(standard_data)

X=standard_data

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size = 1/3, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

print(X.shape, X_train.shape, X_test.shape)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
#trainig
classifier.fit(X_train, y_train)
#testing
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size = 1/3, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

print(X.shape, X_train.shape, X_test.shape)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
#trainig
classifier.fit(X_train, y_train)
#testing
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
# training
model.fit(X_train, y_train)
# testing
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
# training
model.fit(X_train, y_train)
# testing
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))

from xgboost import XGBClassifier
model = XGBClassifier(n_jobs=-1)
# training
model.fit(X_train, y_train)
# testing
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("F1 Score:",f1_score(y_test, y_pred))